{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías necesarias\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, BertConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'Abstracts_filtrados.csv' cargado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# 1. Cargar el archivo CSV con los datos\n",
    "try:\n",
    "    df = pd.read_csv('Abstracts_filtrados.csv')\n",
    "    print(\"Archivo 'Abstracts_filtrados.csv' cargado exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Archivo 'Abstracts_filtrados.csv' no encontrado. Asegúrate de que está en el directorio de trabajo.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Función para limpiar el texto\n",
    "def limpiar_texto(texto):\n",
    "    texto_limpio = BeautifulSoup(texto, \"html.parser\").get_text()  # Eliminar etiquetas HTML\n",
    "    texto_limpio = re.sub(r'\\s+', ' ', texto_limpio)  # Eliminar múltiples espacios\n",
    "    texto_limpio = re.sub(r'[^a-zA-Z0-9\\s]', '', texto_limpio)  # Eliminar caracteres especiales\n",
    "    if detect(texto_limpio) == 'en':  # Verificar si el texto está en inglés\n",
    "        return texto_limpio.strip()\n",
    "    return ''\n",
    "\n",
    "# Aplicar la función de limpieza al DataFrame\n",
    "df['Abstract_Limpio'] = df['Abstract'].apply(limpiar_texto)\n",
    "\n",
    "# Guardar el DataFrame limpio\n",
    "df.to_csv('Abstracts_limpios.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jhonr\\anaconda3\\envs\\NPL_ODS\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3. Cargar el tokenizador de BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Dividir el dataset en entrenamiento y evaluación\n",
    "X = df['Abstract_Limpio'].tolist()\n",
    "y = df['ODS'].tolist()\n",
    "\n",
    "# Convertir etiquetas a valores numéricos (si es necesario)\n",
    "# Asumiendo que y es una lista de strings con las clases de ODS\n",
    "unique_labels = list(set(y))\n",
    "label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "y = [label_to_id[label] for label in y]\n",
    "\n",
    "# Dividir en conjunto de entrenamiento y evaluación\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenizar los textos para entrenamiento y evaluación\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=512)\n",
    "eval_encodings = tokenizer(X_eval, truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Crear el Dataset personalizado para PyTorch\n",
    "class ODSDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Crear los datasets de entrenamiento y evaluación\n",
    "train_dataset = ODSDataset(train_encodings, y_train)\n",
    "eval_dataset = ODSDataset(eval_encodings, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Configurar el modelo con el número correcto de etiquetas\n",
    "num_labels = len(unique_labels)  # Obtener el número de clases únicas\n",
    "config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "config.hidden_dropout_prob = 0.3  # Incrementar dropout para regularización\n",
    "config.num_labels = num_labels  # Ajustar el número de etiquetas\n",
    "\n",
    "# Cargar el modelo con la configuración ajustada\n",
    "model = BertForSequenceClassification(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de clases únicas: 11\n",
      "Etiquetas: ['\\tODS 9 - Industria, Innovación e Infraestructura', 'ODS 7 - Energía Asequible y No Contaminante', 'ODS 2: Hambre Cero', 'ODS 13 - Acción por el Clima', '\\tODS 3 - Salud y Bienestar', 'ODS 8 - Trabajo Decente y Crecimiento Económico', '\\tODS 4 - Educación de Calidad', 'ODS 3 - Salud y Bienestar', 'ODS 2 - Hambre Cero', 'ODS 6 - Agua Limpia y Saneamiento', 'ODS 9 - Industria, Innovación e Infraestructura']\n"
     ]
    }
   ],
   "source": [
    "# 6. Configurar los argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',            # Directorio donde se guardan los resultados\n",
    "    evaluation_strategy=\"epoch\",       # Evalúa el modelo al final de cada época\n",
    "    per_device_train_batch_size=8,     # Tamaño del batch por dispositivo\n",
    "    per_device_eval_batch_size=8,      # Tamaño del batch para evaluación\n",
    "    num_train_epochs=5,                # Número de épocas de entrenamiento\n",
    "    learning_rate=2e-5,                # Tasa de aprendizaje baja\n",
    "    weight_decay=0.01,                 # Usar decay para evitar sobreajuste\n",
    "    logging_dir='./logs',              # Directorio de logs\n",
    "    logging_steps=10,                  # Loguear cada 10 pasos\n",
    "    warmup_steps=500                   # Pasos de calentamiento para estabilidad\n",
    ")\n",
    "\n",
    "# Verificar el número de clases en las etiquetas\n",
    "print(f\"Total de clases únicas: {num_labels}\")\n",
    "print(f\"Etiquetas: {unique_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Definir una función para calcular métricas personalizadas\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Inicializar el entrenador con las métricas personalizadas\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics   # Función personalizada para calcular métricas\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                              \n",
      "\n",
      "  0%|          | 0/30 [05:00<?, ?it/s]       \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2088963985443115, 'eval_accuracy': 0.5833333333333334, 'eval_precision': 0.7569444444444445, 'eval_recall': 0.5833333333333334, 'eval_f1': 0.4298245614035088, 'eval_runtime': 6.7908, 'eval_samples_per_second': 1.767, 'eval_steps_per_second': 0.295, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/30 [06:02<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3435, 'grad_norm': 15.836101531982422, 'learning_rate': 4.0000000000000003e-07, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                               \n",
      "\n",
      "  0%|          | 0/30 [06:38<?, ?it/s]       \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1262805461883545, 'eval_accuracy': 0.5833333333333334, 'eval_precision': 0.7569444444444445, 'eval_recall': 0.5833333333333334, 'eval_f1': 0.4298245614035088, 'eval_runtime': 6.7298, 'eval_samples_per_second': 1.783, 'eval_steps_per_second': 0.297, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                               \n",
      "\n",
      "  0%|          | 0/30 [08:14<?, ?it/s]       \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0009539127349854, 'eval_accuracy': 0.5833333333333334, 'eval_precision': 0.7569444444444445, 'eval_recall': 0.5833333333333334, 'eval_f1': 0.4298245614035088, 'eval_runtime': 6.3378, 'eval_samples_per_second': 1.893, 'eval_steps_per_second': 0.316, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/30 [08:43<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.175, 'grad_norm': 17.596250534057617, 'learning_rate': 8.000000000000001e-07, 'epoch': 3.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                               \n",
      "\n",
      "  0%|          | 0/30 [09:47<?, ?it/s]       \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8590679168701172, 'eval_accuracy': 0.5833333333333334, 'eval_precision': 0.7569444444444445, 'eval_recall': 0.5833333333333334, 'eval_f1': 0.4298245614035088, 'eval_runtime': 6.6905, 'eval_samples_per_second': 1.794, 'eval_steps_per_second': 0.299, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/30 [11:17<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0471, 'grad_norm': 15.62112045288086, 'learning_rate': 1.2000000000000002e-06, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                               \n",
      "\n",
      "  0%|          | 0/30 [11:26<?, ?it/s]       \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 30/30 [07:59<00:00, 15.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7412573099136353, 'eval_accuracy': 0.5833333333333334, 'eval_precision': 0.7569444444444445, 'eval_recall': 0.5833333333333334, 'eval_f1': 0.4298245614035088, 'eval_runtime': 6.6311, 'eval_samples_per_second': 1.81, 'eval_steps_per_second': 0.302, 'epoch': 5.0}\n",
      "{'train_runtime': 479.592, 'train_samples_per_second': 0.49, 'train_steps_per_second': 0.063, 'train_loss': 2.1885454177856447, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30, training_loss=2.1885454177856447, metrics={'train_runtime': 479.592, 'train_samples_per_second': 0.49, 'train_steps_per_second': 0.063, 'total_flos': 59058301113990.0, 'train_loss': 2.1885454177856447, 'epoch': 5.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9. Iniciar el entrenamiento\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de evaluación: {'eval_loss': 1.7412573099136353, 'eval_accuracy': 0.5833333333333334, 'eval_precision': 0.7569444444444445, 'eval_recall': 0.5833333333333334, 'eval_f1': 0.4298245614035088, 'eval_runtime': 6.6583, 'eval_samples_per_second': 1.802, 'eval_steps_per_second': 0.3, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 10. Evaluar el modelo\n",
    "results = trainer.evaluate()\n",
    "print(\"Resultados de evaluación:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# 11. Obtener las predicciones del conjunto de evaluación\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "y_preds = np.argmax(predictions.predictions, axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
